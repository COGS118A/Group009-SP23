{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from Logit.ipynb\n",
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    url = 'https://raw.githubusercontent.com/COGS118A/Group009-SP23/main/clean_data.csv'\n",
    "    data = pd.read_csv(url)\n",
    "    data = data.dropna()\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code performs the following tasks:\n",
    "\n",
    "1. `get_dataset(data)`: This function takes a `data` input and prepares the dataset for training. It separates the features and the target variable, performs a train-test split with a test size of 0.3 and a random state of 42, and returns the following variables: `X` (features), `y` (target variable), `X_train` (training set features), `X_test` (testing set features), `y_train` (training set target variable), and `y_test` (testing set target variable).\n",
    "\n",
    "2. `get_models(X_train, X_test, y_train, y_test)`: This function takes the training and testing sets and creates multiple logistic regression models with different hyperparameters. It iterates five times, each time creating a logistic regression model with increasing values for the maximum number of iterations (`max_iter`) and regularization strength (`C`). The models are trained on the `X_train` and `y_train` data, and their predictions are obtained for the `X_test` data. The function returns a dictionary (`models`) where the keys are the model names (e.g., 'Logistic Regression 1', 'Logistic Regression 2', etc.) and the values are dictionaries containing the respective model and its predictions.\n",
    "\n",
    "3. `get_SV(models, X_train, X_test, y_train, y_test)`: This function takes the models generated in the previous step along with the training and testing sets and creates a soft voting classifier. It extracts the individual models and their names from the `models` dictionary and uses them as estimators for the voting classifier. The voting classifier is then trained on the `X_train` and `y_train` data, and its predictions are obtained for the `X_test` data. The function adds the voting classifier to the `models` dictionary with the key 'Soft Voting' and returns the updated dictionary.\n",
    "\n",
    "4. `get_accuracy(model, X, y)`: This function calculates the accuracy scores of a given model using cross-validation. It uses a repeated stratified k-fold cross-validation with 10 splits and 3 repeats. The function returns an array of accuracy scores for each fold.\n",
    "\n",
    "The main part of the code then executes the following steps:\n",
    "\n",
    "1. Calls `get_dataset(get_data())` to obtain the dataset and assign the returned values to variables `X`, `y`, `X_train`, `X_test`, `y_train`, and `y_test`.\n",
    "\n",
    "2. Calls `get_models(X_train, X_test, y_train, y_test)` to generate multiple logistic regression models and store them in the `models` dictionary.\n",
    "\n",
    "3. Calls `get_SV(models, X_train, X_test, y_train, y_test)` to add a soft voting classifier to the `models` dictionary.\n",
    "\n",
    "4. Initializes empty lists `results` and `names`.\n",
    "\n",
    "5. Iterates over the items in the `models` dictionary and for each model, performs cross-validation using `get_accuracy(model, X, y)`. The accuracy scores are appended to the `results` list, and the model names are appended to the `names` list. Additionally, the mean and standard deviation of the accuracy scores are printed for each model.\n",
    "\n",
    "6. Uses `pyplot` to create a boxplot of the accuracy scores (`results`) for each model (`names`). The boxplot is displayed using `pyplot.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16467, 216)\n",
      ">Logistic Regression 1 0.845 (0.008)\n",
      ">Logistic Regression 2 0.845 (0.008)\n",
      ">Logistic Regression 3 0.845 (0.008)\n",
      ">Logistic Regression 4 0.844 (0.009)\n",
      ">Logistic Regression 5 0.844 (0.009)\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(data):\n",
    "    X = data.drop('good_outcome', axis=1)\n",
    "    y = data['good_outcome']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    return X, y, X_train, X_test, y_train, y_test\n",
    "\n",
    "def get_models(X_train, X_test, y_train, y_test):\n",
    "    models = {}\n",
    "\n",
    "    for i in range(5):\n",
    "        model_name = f'Logistic Regression {i+1}'  # Generate a unique model name\n",
    "\n",
    "        model = LogisticRegression(max_iter=(i+1)*1000, C=(i+1)*0.1)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        models[model_name] = {\n",
    "            'model': model,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "    return models\n",
    "\n",
    "def get_SV(models,X_train, X_test, y_train, y_test):\n",
    "    individual_models = []\n",
    "\n",
    "    for model_name, model_info in models.items():\n",
    "        individual_models.append((model_name, model_info['model']))\n",
    "\n",
    "    voting_classifier = VotingClassifier(estimators=individual_models, voting='soft')\n",
    "    voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_voting = voting_classifier.predict(X_test)\n",
    "\n",
    "    models['Soft Voting'] = {\n",
    "        'model': voting_classifier,\n",
    "        'predictions': y_pred_voting\n",
    "    }\n",
    "    return models\n",
    "\n",
    "def get_accuracy(model, X, y): #cross valid\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "X, y, X_train, X_test, y_train, y_test = get_dataset(get_data())\n",
    "\n",
    "models = get_models(X_train, X_test, y_train, y_test)\n",
    "models = get_SV(models, X_train, X_test, y_train, y_test)\n",
    "\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    model = model_info['model']\n",
    "    scores = get_accuracy(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=[1,2,3,4,5, \"Soft Voting\"], showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we had to do it by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def soft_voting(accuracies):\n",
    "    weights = accuracies / np.sum(accuracies)\n",
    "    predictions = np.random.randint(0, 2, size=(len(accuracies), 10))\n",
    "    ensemble_prediction = np.average(predictions, axis=0, weights=weights)\n",
    "    \n",
    "    return np.argmax(ensemble_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
